
@misc{pedamonti_comparison_2018,
	title = {Comparison of non-linear activation functions for deep neural networks on {MNIST} classification task},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/1804.02763},
	doi = {10.48550/ARXIV.1804.02763},
	abstract = {Activation functions play a key role in neural networks so it becomes fundamental to understand their advantages and disadvantages in order to achieve better performances. This paper will first introduce common types of non linear activation functions that are alternative to the well known sigmoid function and then evaluate their characteristics. Moreover deeper neural networks will be analysed because they positively influence the final performances compared to shallower networks. They also strictly depend on the weight initialisation hence the effect of drawing weights from Gaussian and uniform distribution will be analysed making particular attention on how the number of incoming and outgoing connection to a node influence the whole network.},
	urldate = {2024-07-25},
	publisher = {arXiv},
	author = {Pedamonti, Dabal},
	year = {2018},
	note = {Version Number: 1},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@misc{lecun_mnist_2012,
	title = {The {MNIST} {Database} of {Handwritten} {Digits}},
	url = {https://yann.lecun.com/exdb/mnist/},
	author = {LeCun, Y. and Cortes, C. and Burges, C.J.C.},
	year = {2012},
}

@inproceedings{deng_deep_2011,
	title = {Deep {Convex} {Network}: {A} {Scalable} {Architecture} for {Speech} {Pattern} {Classification}},
	url = {https://www.microsoft.com/en-us/research/publication/deep-convex-network-a-scalable-architecture-for-speech-pattern-classification/},
	abstract = {We recently developed context-dependent DNN-HMM (DeepNeural-Net/Hidden-Markov-Model) for large-vocabulary speech recognition. While achieving impressive recognition error rate reduction, we face the insurmountable problem of scalability in dealing with virtually unlimited amount of training data available nowadays. To overcome the scalability challenge, we have designed the deep convex network (DCN) architecture. The learning problem in DCN is convex within each module. Additional structure-exploited fine tuning further improves the quality of DCN. The full learning in DCN is batch-mode based instead of stochastic, naturally lending it amenable to parallel training that can be distributed over many machines. Experimental results on both MNIST and TIMIT tasks evaluated thus far demonstrate superior performance of DCN over the DBN (Deep Belief Network) counterpart that forms the basis of the DNN. The superiority is reflected not only in training scalability and CPU-only computation, but more importantly in classification accuracy in both tasks.},
	booktitle = {Interspeech},
	publisher = {International Speech Communication Association},
	author = {Deng, Li and Yu, Dong},
	month = aug,
	year = {2011},
	note = {Edition: Interspeech},
	file = {Full Text:/home/shiba/Zotero/storage/42Z35U7B/Deng and Yu - 2011 - Deep Convex Network A Scalable Architecture for S.pdf:application/pdf},
}

@misc{hasanpour_lets_2016,
	title = {Lets keep it simple, {Using} simple architectures to outperform deeper and more complex architectures},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1608.06037},
	doi = {10.48550/ARXIV.1608.06037},
	abstract = {Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded systems or systems with computational and memory limitations. We achieved state-of-the-art result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and competitive results on CIFAR100 and SVHN. We also outperformed the much larger and deeper architectures such as VGGNet and popular variants of ResNets among others on the ImageNet dataset. Models are made available at: https://github.com/Coderx7/SimpleNet},
	urldate = {2024-07-25},
	publisher = {arXiv},
	author = {Hasanpour, Seyyed Hossein and Rouhani, Mohammad and Fayyaz, Mohsen and Sabokrou, Mohammad},
	year = {2016},
	note = {Version Number: 8},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, Neural and Evolutionary Computing (cs.NE)},
}

@article{ciresan_deep_2010,
	title = {Deep, {Big}, {Simple} {Neural} {Nets} for {Handwritten} {Digit} {Recognition}},
	volume = {22},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/22/12/3207-3220/7596},
	doi = {10.1162/NECO_a_00052},
	abstract = {Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35\% error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.},
	language = {en},
	number = {12},
	urldate = {2024-07-25},
	journal = {Neural Computation},
	author = {Cireşan, Dan Claudiu and Meier, Ueli and Gambardella, Luca Maria and Schmidhuber, Jürgen},
	month = dec,
	year = {2010},
	pages = {3207--3220},
	file = {Submitted Version:/home/shiba/Zotero/storage/4F8NHW2D/Cireşan et al. - 2010 - Deep, Big, Simple Neural Nets for Handwritten Digi.pdf:application/pdf},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	number = {11},
	urldate = {2024-07-25},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324},
	file = {Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:/home/shiba/Zotero/storage/Y3TE48F2/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf},
}
